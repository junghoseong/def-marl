# EFXplorer í•™ìŠµ ë£¨í”„ ë¶„ì„ ë° ê°œì„  ì‚¬í•­

**ìµœì¢… ì—…ë°ì´íŠ¸**: í•™ìŠµ ê°€ëŠ¥ ìƒíƒœ í™•ì¸ ì™„ë£Œ âœ…

## 1. í˜„ì¬ êµ¬í˜„ ìƒíƒœ ìš”ì•½

### 1.1 EFXplorerì˜ í•µì‹¬ íŠ¹ì§•
- **Vi (Intrinsic Value) ë„¤íŠ¸ì›Œí¬ ì œê±°**: Ve (Extrinsic Value)ë§Œ ì‚¬ìš©
- **Conservative Exploration Framework**: 
  - Final Advantage: `A_t^{(T)} = min{A_t^ext, A_{t+1}^ext, ..., A_T^ext, S_t}`
  - S_t = R_t^int - z_t (intrinsic return - budget)
- **z-dynamics**: `z_{t+1} = z_t - Î”_t^team-int` (rollout ì¤‘ ìë™ ì—…ë°ì´íŠ¸)
- **Team-level intrinsic reward**: `Î”_t^team-int = Î£_i r_{i,t}^int = Î£_i (-log Ï€_i * intrinsic_coef)`
- **z ì—…ë°ì´íŠ¸ ì „ëµ**: ì´ì „ rolloutì˜ intrinsic reward í•©ì˜ í‰ê· ìœ¼ë¡œ ì„¤ì • (EMA ë°©ì‹ì€ ì£¼ì„ì²˜ë¦¬)
- **ì´ˆê¸° z**: 0ìœ¼ë¡œ ì´ˆê¸°í™”
- **í‰ê°€ ì‹œ z**: 0ìœ¼ë¡œ ì„¤ì • (conditioning)

### 1.2 DefMARLê³¼ì˜ ì°¨ì´ì 
| í•­ëª© | DefMARL | EFXplorer |
|------|---------|-----------|
| Value Networks | Vl (extrinsic), Vh (constraint) | Ve (extrinsic)ë§Œ |
| z ì—…ë°ì´íŠ¸ | Root finderë¡œ optimal z ê³„ì‚° | ì´ì „ rolloutì˜ intrinsic reward í•© |
| z ì´ˆê¸°í™” | `get_opt_z`ë¡œ ê³„ì‚° | 0ìœ¼ë¡œ ì´ˆê¸°í™” |
| Test ì‹œ z ì„ íƒ | `get_opt_z` ì‚¬ìš© | z=0 ì‚¬ìš© |

---

## 2. í•™ìŠµ ë£¨í”„ êµ¬ì¡° ë¶„ì„

### 2.1 í˜„ì¬ í•™ìŠµ ë£¨í”„ (trainer.py)
```python
for step in range(0, self.steps + 1):
    # 1. í‰ê°€ (eval_intervalë§ˆë‹¤)
    if step % self.eval_interval == 0:
        # test_opt_fnì€ get_opt_zë¥¼ ì‚¬ìš© (EFXplorerì—ëŠ” ì—†ìŒ)
        test_rollouts = test_opt_fn(self.algo.params, test_keys)
    
    # 2. ë°ì´í„° ìˆ˜ì§‘
    rollouts = self.algo.collect(self.algo.params, key_x0)
    
    # 3. ì•Œê³ ë¦¬ì¦˜ ì—…ë°ì´íŠ¸
    update_info = self.algo.update(rollouts, step)
```

### 2.2 EFXplorerì˜ update ë©”ì„œë“œ íë¦„
```python
def update(self, rollout: Rollout, step: int) -> dict:
    # 1. PPO epoch ë°˜ë³µ
    for i_epoch in range(self.epoch_ppo):
        # 2. update_inner í˜¸ì¶œ
        critic_train_state, policy_train_state, update_info = self.update_inner(...)
    
    # âŒ z ì—…ë°ì´íŠ¸ ì—†ìŒ!
    return update_info
```

---

## 3. ë°œê²¬ëœ ë¬¸ì œì  ë° ë¯¸í¡í•œ ë¶€ë¶„

### 3.1 âœ… **ì™„ë£Œ: z ì—…ë°ì´íŠ¸ êµ¬í˜„**

**êµ¬í˜„ ìƒíƒœ**: âœ… ì™„ë£Œ

**êµ¬í˜„ ë‚´ìš©**:
- `update_z` ë©”ì„œë“œ: ì´ì „ rolloutì˜ intrinsic reward í•©ì˜ í‰ê· ìœ¼ë¡œ z ì„¤ì •
- `update` ë©”ì„œë“œì—ì„œ z ì—…ë°ì´íŠ¸ ë¡œì§ ì¶”ê°€
- ì´ˆê¸° z = 0ìœ¼ë¡œ ì„¤ì • (`__init__`ì—ì„œ `self.z_current = jnp.zeros((self.n_agents, 1))`)

**í˜„ì¬ ì½”ë“œ** (`efxplorer.py:561-591`):
```python
def update_z(self, rollout: Rollout, z_old: Float[Array, "a 1"]) -> Float[Array, "a 1"]:
    """
    Update budget z based on previous rollout's intrinsic reward sum
    
    z_new = sum of intrinsic rewards from previous rollout
    """
    # Compute total intrinsic reward per trajectory
    intrinsic_return_per_traj = rollout.intrinsic_rewards.sum(axis=(1, 2))  # (b,)
    intrinsic_return_mean = intrinsic_return_per_traj.mean()
    
    # Set z to the mean intrinsic return (shared across all agents)
    z_new = jnp.full((self.n_agents, 1), intrinsic_return_mean)
    z_new = jnp.clip(z_new, -self._env.reward_max, -self._env.reward_min)
    
    # EMA approach (commented out - design choice)
    # lr_z = 1e-3
    # z_update = lr_z * intrinsic_return_mean
    # z_new = z_old + z_update
    
    return z_new
```

**update ë©”ì„œë“œ** (`efxplorer.py:321-328`):
```python
# Update z based on previous rollout's intrinsic reward sum
z_old = self.z_current
self.z_current = self.update_z(rollout, z_old)

# Add z update info to logging
update_info['z/mean'] = self.z_current.mean()
update_info['z/change'] = (self.z_current - z_old).mean()
update_info['z/value'] = self.z_current[0, 0]
```

---

### 3.2 âœ… **ì™„ë£Œ: get_opt_z ë©”ì„œë“œ êµ¬í˜„**

**êµ¬í˜„ ìƒíƒœ**: âœ… ì™„ë£Œ

**êµ¬í˜„ ë‚´ìš©**:
- í‰ê°€ ì‹œ z=0ì„ ë°˜í™˜í•˜ë„ë¡ êµ¬í˜„
- `trainer.py`ì˜ `hasattr` ì²´í¬ì™€ í˜¸í™˜

**í˜„ì¬ ì½”ë“œ** (`efxplorer.py:600-615`):
```python
def get_opt_z(
    self,
    graph: GraphsTuple,
    rnn_state: Array,
    params: Optional[Params] = None
) -> Tuple[Array, Array]:
    """
    Get z for evaluation. For EFXplorer, we use z=0 for evaluation.
    
    Returns
    -------
    z: Array
        Budget z set to 0 for evaluation (shape: (n_agents, 1))
    rnn_state: Array
        Unchanged RNN state
    """
    # For evaluation, use z=0 (initial budget)
    z = jnp.zeros((self.n_agents, 1))
    return z, rnn_state
```

---

### 3.3 âœ… **ì™„ë£Œ: update_policyì˜ shape assertion ìˆ˜ì •**

**êµ¬í˜„ ìƒíƒœ**: âœ… ì™„ë£Œ

**ìˆ˜ì • ë‚´ìš©**:
- `gaes.shape` assertionì„ `(b, T, n_agents)`ë¡œ ìˆ˜ì •

**í˜„ì¬ ì½”ë“œ** (`efxplorer.py:507-509`):
```python
# Final advantages are agent-wise (b, T, a)
assert gaes.shape == (rollout.actions.shape[0], rollout.actions.shape[1], self.n_agents), \
    f"Expected gaes shape {(rollout.actions.shape[0], rollout.actions.shape[1], self.n_agents)}, got {gaes.shape}"
```

---

### 3.4 ğŸŸ¡ **ìš°ì„ ìˆœìœ„ ì¤‘ê°„: z ì´ˆê¸°í™” ì „ëµ**

**ë¬¸ì œ**: 
- Rollout ì‹œ zê°€ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ë¨ (`rollout_efxplorer:106-111`)
- í•™ìŠµëœ zë¥¼ í™œìš©í•˜ì§€ ì•ŠìŒ

**í˜„ì¬ ì½”ë“œ** (`trainer/utils.py:106-111`):
```python
z0 = jax.random.uniform(key_z0, (1, 1), minval=-env.reward_max, maxval=-env.reward_min)
rng = jax.random.uniform(z_key, (1, 1))
z0 = jnp.where(rng > 0.7, -env.reward_max, z0)
z0 = jnp.where(rng < 0.2, -env.reward_min, z0)
```

**ê°œì„  ë°©ì•ˆ**:
- í•™ìŠµ ì´ˆê¸°ì—ëŠ” ëœë¤, ì´í›„ì—ëŠ” í•™ìŠµëœ z ì‚¬ìš©
- ë˜ëŠ” zë¥¼ stateë¡œ ê´€ë¦¬í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸

---

### 3.5 âœ… **ì™„ë£Œ: update_innerì˜ ìˆ˜ì • ì‚¬í•­**

**êµ¬í˜„ ìƒíƒœ**: âœ… ì™„ë£Œ

**ìˆ˜ì • ë‚´ìš©**:
1. `scan_value` í˜¸ì¶œ ì‹œ `rollout.graph` ëŒ€ì‹  `rollout` ì „ì²´ ì „ë‹¬
2. `final_value_fn`ì—ì„œ z ì „ë‹¬ ì¶”ê°€
3. `final_Ve` shape ì²˜ë¦¬ (squeeze ì¶”ê°€)

**í˜„ì¬ ì½”ë“œ** (`efxplorer.py:392-405`):
```python
# 1. Compute Ve
bT_Ve, rnn_states_Ve, final_rnn_states_Ve = jax_vmap(
    ft.partial(self.scan_value,
               init_rnn_state_Ve=self.init_Vl_rnn_state,
               critic_params=critic_train_state.params)
)(rollout)  # values: (b, T)

# 2. Compute Final Ve
def final_value_fn(graph, zs, rnn_state):
    # Use the last z value from the trajectory
    z_final = zs[-1][0][None, :] if zs is not None else None
    return self.critic.get_value(critic_train_state.params, tree_index(graph, -1), rnn_state, z_final)

final_Ve, _ = jax.vmap(final_value_fn)(rollout.next_graph, rollout.zs, final_rnn_states_Ve)
final_Ve = final_Ve.squeeze()
bTp1_Ve = jnp.concatenate([bT_Ve, final_Ve[:, None]], axis=1) # (b, T+1)
```

### 3.6 âœ… **ì™„ë£Œ: update_criticì˜ ìˆ˜ì • ì‚¬í•­**

**êµ¬í˜„ ìƒíƒœ**: âœ… ì™„ë£Œ

**ìˆ˜ì • ë‚´ìš©**:
- `update_critic`ì—ì„œ chunkë¡œ ë‚˜ëˆˆ rollout ì „ì²´ë¥¼ ì „ë‹¬í•˜ë„ë¡ ìˆ˜ì •

**í˜„ì¬ ì½”ë“œ** (`efxplorer.py:468-478`):
```python
# Create chunked rollout
bcT_rollout = jax.tree.map(lambda x: x[:, rnn_chunk_ids], rollout)
rnn_state_inits = jnp.zeros_like(rnn_states[:, rnn_chunk_ids[:, 0]])

def get_value_loss(params):
    values, value_rnn_states, final_value_rnn_states = jax.vmap(jax.vmap(
        ft.partial(self.scan_value,
                   init_rnn_state_Ve=rnn_state_inits,
                   critic_params=params)
    ))(bcT_rollout)  # values: (b, n_chunks, T_chunk)
    values = values.reshape((values.shape[0], -1))
    loss_critic = optax.l2_loss(values, targets).mean()
    return loss_critic
```

---

### 3.7 ğŸŸ¢ **ìš°ì„ ìˆœìœ„ ë‚®ìŒ: z-dynamics ê²€ì¦**

**ìƒíƒœ**: í•™ìŠµ ì¤‘ ê²€ì¦ ê°€ëŠ¥

**ê²€ì¦ ë°©ë²•**:
```python
# update_innerì—ì„œ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
z_diff = rollout.zs[:, 1:] - rollout.zs[:, :-1]  # (b, T-1, a, 1)
Delta_team_int_expected = -rollout.intrinsic_rewards.sum(axis=-1)[:, :-1]  # (b, T-1)
# z_diffì™€ Delta_team_int_expected ë¹„êµ
```

---

## 4. í•™ìŠµ ë£¨í”„ ê°œì„  ì œì•ˆ

### 4.1 ìˆ˜ì •ëœ update ë©”ì„œë“œ

```python
def update(self, rollout: Rollout, step: int) -> dict:
    key, self.key = jr.split(self.key)
    
    update_info = {}
    assert rollout.dones.shape[0] * rollout.dones.shape[1] >= self.batch_size
    
    # PPO ì—…ë°ì´íŠ¸
    for i_epoch in range(self.epoch_ppo):
        idx = np.arange(rollout.dones.shape[0])
        np.random.shuffle(idx)
        rnn_chunk_ids = jnp.arange(rollout.dones.shape[1])
        rnn_chunk_ids = jnp.array(jnp.array_split(rnn_chunk_ids, rollout.dones.shape[1] // self.rnn_step))
        batch_idx = jnp.array(jnp.array_split(idx, idx.shape[0] // (self.batch_size // rollout.dones.shape[1])))
        
        critic_train_state, policy_train_state, update_info = self.update_inner(
            self.critic_train_state,
            self.policy_train_state,
            rollout,
            batch_idx,
            rnn_chunk_ids
        )
        self.critic_train_state = critic_train_state
        self.policy_train_state = policy_train_state
    
    # âœ… ì¶”ê°€: z ì—…ë°ì´íŠ¸ (Outer Loop)
    if not hasattr(self, 'z_current'):
        # ì´ˆê¸° z: rolloutì˜ í‰ê·  z ì‚¬ìš©
        self.z_current = rollout.zs.mean(axis=(0, 1))  # (a, 1)
    
    # z ì—…ë°ì´íŠ¸ (ë…¼ë¬¸ì˜ Outer Loop)
    z_old = self.z_current
    self.z_current = self.update_z(rollout, z_old)
    
    # z ì—…ë°ì´íŠ¸ ì •ë³´ ì¶”ê°€
    update_info['z/mean'] = self.z_current.mean()
    update_info['z/change'] = (self.z_current - z_old).mean()
    
    if self.use_prev_init:
        self.memory = rollout
    
    return update_info
```

### 4.2 ì¶”ê°€í•  get_opt_z ë©”ì„œë“œ

```python
def get_opt_z(
    self,
    graph: GraphsTuple,
    rnn_state: Array,
    params: Optional[Params] = None
) -> Tuple[Array, Array]:
    """
    í‰ê°€ ì‹œ ì‚¬ìš©í•  z ë°˜í™˜
    EFXplorerì˜ ê²½ìš° í•™ìŠµëœ z_current ì‚¬ìš©
    """
    if hasattr(self, 'z_current'):
        z = self.z_current
    else:
        # ê¸°ë³¸ê°’: ì¤‘ê°„ê°’
        z_mid = (-self._env.reward_max - self._env.reward_min) / 2
        z = jnp.array([[z_mid]]).repeat(self.n_agents, axis=0)
    
    return z, rnn_state
```

---

## 5. ì²´í¬ë¦¬ìŠ¤íŠ¸

### 5.1 í•„ìˆ˜ ìˆ˜ì • ì‚¬í•­ âœ…
- [x] `update` ë©”ì„œë“œì— z ì—…ë°ì´íŠ¸ ë¡œì§ ì¶”ê°€ âœ…
- [x] `get_opt_z` ë©”ì„œë“œ êµ¬í˜„ âœ…
- [x] `update_policy`ì˜ shape assertion ìˆ˜ì • âœ…
- [x] `update_inner`ì—ì„œ `scan_value` í˜¸ì¶œ ìˆ˜ì • (rollout ì „ì²´ ì „ë‹¬) âœ…
- [x] `final_value_fn`ì—ì„œ z ì „ë‹¬ ì¶”ê°€ âœ…
- [x] `update_critic`ì—ì„œ chunk rollout ì „ë‹¬ ìˆ˜ì • âœ…

### 5.2 í•™ìŠµ ê°€ëŠ¥ì„± í™•ì¸ âœ…
- [x] `make_algo`ì—ì„œ `efxplorer` ë“±ë¡ í™•ì¸ âœ…
- [x] `train.py`ì—ì„œ í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ë‹¬ í™•ì¸ âœ…
- [x] `intrinsic_coef` ê¸°ë³¸ê°’ ì„¤ì • í™•ì¸ (1.0) âœ…
- [x] ë¶ˆí•„ìš”í•œ ì¸ì (`Vh_gnn_layers`, `lagr_init`, `lr_lagr`)ëŠ” `**kwargs`ë¡œ ì²˜ë¦¬ âœ…

### 5.3 ê¶Œì¥ ê°œì„  ì‚¬í•­ (ì„ íƒì )
- [ ] z-dynamics ê²€ì¦ ì½”ë“œ ì¶”ê°€ (í•™ìŠµ ì¤‘ ê²€ì¦ ê°€ëŠ¥)
- [ ] z ì—…ë°ì´íŠ¸ ë¡œê¹… ê°•í™” (í˜„ì¬ ê¸°ë³¸ ë¡œê¹… êµ¬í˜„ë¨)
- [ ] z í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (í˜„ì¬ëŠ” intrinsic reward sum ì§ì ‘ ì‚¬ìš©)

---

## 6. ì°¸ê³ : ë°ì´í„° íë¦„ & í…ì„œ ì°¨ì› (rolloutâ†’update)

### 6.1 Rollout (trainer/utils.py)
- `rollout_efxplorer` ì¶œë ¥
  - `rewards`: (b, T, a) ë˜ëŠ” (b, T) â†’ ì‚¬ìš© ì „ (b, T, a)ë¡œ í™•ì¥
  - `intrinsic_rewards`: (b, T, a), per-agent `-log_pi * intrinsic_coef`
  - `zs`: (b, T, a, 1), shared z but tiled per agent
  - `dones`: (b, T)

### 6.2 update_inner (efxplorer.py)
- ì…ë ¥ shapes:
  - `bT_Ve`: (b, T, 1), `bTp1_Ve`: (b, T+1, 1)
  - `rollout.rewards`: (b, T, a)ë¡œ ë³´ì •í•˜ì—¬ ì‚¬ìš©
  - `bT_Delta_team_int`: (b, T) = intrinsic_rewards.sum(axis=-1)
- Ae ê³„ì‚° (per env):
  - rewards_sum = rewards.sum(-1) â†’ (T,)
  - `compute_gae_single(values.squeeze(-1), rewards_sum, dones, next_values.squeeze(-1))`
  - Ae: (T, a) via repeat over agents
- A_final:
  - `compute_conservative_exploration_gae(Ae (T,a), Î”_team_int (T,), z (T,a))`
  - ì¶œë ¥: (T, a), ë°°ì¹˜ vmap í›„ (b, T, a)
- ì •ê·œí™”:
  - `(b, T, a)` ë‹¨ìœ„ë¡œ í‰ê· /í‘œì¤€í¸ì°¨

### 6.3 update_critic (efxplorer.py)
- ì…ë ¥:
  - `targets`: (b, T, 1) â†’ squeeze â†’ (b, T) â†’ reshape (b, *)
  - `values`: (b, n_chunks, T_chunk) â†’ reshape (b, *)
- ì†ì‹¤:
  - `optax.l2_loss(values, targets)`; shape ì •ë ¬ ì´í›„ ì‹¤í–‰

### 6.4 z ì—…ë°ì´íŠ¸
- `update_z`:
  - intrinsic_rewards.sum(axis=(1,2)) â†’ (b,)
  - mean â†’ scalar â†’ z_new = fill((a,1), mean), clip to [-reward_max, -reward_min]
- `get_opt_z`: í‰ê°€ ì‹œ z=0 ë°˜í™˜ (shape (a,1))

### 6.5 ì²´í¬ìš© shape ë¦¬ìŠ¤íŠ¸
- `rollout.rewards`: ê¸°ëŒ€ (b, T, a)
- `rollout.intrinsic_rewards`: (b, T, a)
- `rollout.zs`: (b, T, a, 1)
- `bT_Ve`: (b, T, 1), `bTp1_Ve`: (b, T+1, 1)
- `bT_Delta_team_int`: (b, T)
- `Ae (b, T, a)`, `A_final (b, T, a)`

---

## 7. í•™ìŠµ ì‹¤í–‰
```bash
python train.py --env MPETarget --algo efxplorer -n 3 --obs 3
```

ê²€ì¦ ì‹œ í™•ì¸:
- `z/mean`, `z/change`, `z/value`
- `policy/loss`, `critic/loss` ìˆ˜ë ´
- í•„ìš”ì‹œ rollout ì¤‘ z-dynamics ëª¨ë‹ˆí„°ë§

